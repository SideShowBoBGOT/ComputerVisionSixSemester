'''
Розробити програмний скрипт, що забезпечує ідентифікацію бінарних зображень 6 спеціальних знаків,
заданих матрицею растра. Для ідентифікації синтезувати, навчити та застосувати штучну нейронну мережу в «сирому» вигляді
реалізації матричних операцій.
Обрані символи: &, ∧, ∨, ↵, ∑, ≡.
'''

import numpy as np
import matplotlib.pyplot as plt


#----------------------------------- вхідні дані DataSet масив ------------------------------------
def data_x ():

    '''
    Вхідна частина навчального DataSet масиву
    формування вхідних бінарних даних графічних примітивів
    :return: x - np.array
    '''

    # &
    a = [0, 0, 1, 1, 0, 0,
         0, 1, 0, 0, 1, 0,
         0, 1, 0, 0, 1, 0,
         0, 0, 1, 1, 0, 0,
         0, 1, 0, 0, 1, 1,
         1, 0, 0, 0, 1, 0,
         1, 0, 0, 0, 1, 1,
         0, 1, 1, 1, 0, 0,]
    # ∧
    b = [0, 0, 1, 1, 0, 0,
         0, 0, 1, 1, 0, 0,
         0, 1, 0, 0, 1, 0,
         0, 1, 0, 0, 1, 0,
         0, 1, 0, 0, 1, 0,
         1, 0, 0, 0, 0, 1,
         1, 0, 0, 0, 0, 1,
         1, 0, 0, 0, 0, 1,]
    # ∨
    c = [1, 0, 0, 0, 0, 1,
         1, 0, 0, 0, 0, 1,
         1, 0, 0, 0, 0, 1,
         0, 1, 0, 0, 1, 0,
         0, 1, 0, 0, 1, 0,
         0, 1, 0, 0, 1, 0,
         0, 0, 1, 1, 0, 0,
         0, 0, 1, 1, 0, 0,]
    #  ↵
    d = [0, 0, 0, 0, 0, 1,
         0, 0, 0, 0, 0, 1,
         0, 0, 1, 0, 0, 1,
         0, 1, 0, 0, 0, 1,
         1, 1, 1, 1, 1, 1,
         0, 1, 0, 0, 0, 0,
         0, 0, 1, 0, 0, 0,
         0, 0, 0, 0, 0, 0, ]
    # ∑
    e = [1, 1, 1, 1, 1, 1,
         1, 0, 0, 0, 0, 0,
         0, 1, 0, 0, 0, 0,
         0, 0, 1, 1, 0, 0,
         0, 0, 1, 1, 0, 0,
         0, 1, 0, 0, 0, 0,
         1, 0, 0, 0, 0, 0,
         1, 1, 1, 1, 1, 1, ]
    # ≡
    f = [0, 0, 0, 0, 0, 0,
         1, 1, 1, 1, 1, 1,
         0, 0, 0, 0, 0, 0,
         1, 1, 1, 1, 1, 1,
         0, 0, 0, 0, 0, 0,
         1, 1, 1, 1, 1, 1,
         0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, ]
    # ----- Візуалізація --------
    plt.subplot(2, 3, 1)
    plt.imshow(np.array(a).reshape(8, 6))
    plt.subplot(2, 3, 2)
    plt.imshow(np.array(b).reshape(8, 6))
    plt.subplot(2, 3, 3)
    plt.imshow(np.array(c).reshape(8, 6))
    plt.subplot(2, 3, 4)
    plt.imshow(np.array(d).reshape(8, 6))
    plt.subplot(2, 3, 5)
    plt.imshow(np.array(e).reshape(8, 6))
    plt.subplot(2, 3, 6)
    plt.imshow(np.array(f).reshape(8, 6))
    plt.show()
    # ----- Вхідна частина навчального DataSet масиву --------
    x = [np.array(a).reshape(1, 48), np.array(b).reshape(1, 48), np.array(c).reshape(1, 48), np.array(d).reshape(1, 48),
         np.array(e).reshape(1, 48), np.array(f).reshape(1, 48)]

    return x


def data_y():
    '''
    Вихідна частина навчального DataSet масиву - відповідь
    формування кодових комбінацій бінарних відповідей в просторі 6 значень
    :return: y - np.array
    '''

    # ----- Вихідна частина навчального DataSet масиву --------
    out_abc = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0],
               [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]]
    y = np.array(out_abc)

    return y


# ----------------------------------- конструювання нейромережі ------------------------------------

# ----- функція активності - сігмоід -----
def sigmoid(x):
    '''
    :param x: - np.array DataSet in
    :return: activation function - sigmoid
    '''

    return (1 / (1 + np.exp(-x)))


# ----- конструювання нейронної мережі  -----
def f_forward(x, w1, w2):
    '''
    Головна компонента конструювання нейронної мережі.
    Цим та подальшими відрізняється цей приклад від звичайного пресептрона
    Архітектурні залежності:
    1-й рівень: вхідний рівень (1, 48);
    2-й шар: прихований шар (1, 6);
    3-й шар: вихідний рівень (6, 6).
    Графічне відображення архітектури див.Neural_Networks_numpy_2.jpg

    :param x: np.array -
    :param w1: початкові вагові коефіціенти 1 прошарку (вхідного)
    :param w2: початкові вагові коефіціенти 2 прошарку (прихованого)
    :return: a2 - вектор вихідних параметрів мережі - 3 компоненти
    '''

    # структуру вхідного прошарку визначає простір вхідних параметрів x

    # прихований прошарок
    z1 = x.dot(w1)  # зважені вхідні параметри вхідного прошарку 1
    a1 = sigmoid(z1)  # аддитивна згортка - вихід з прошарку 1 - вхід прошарку 2

    # вихідний прошарок
    z2 = a1.dot(w2)  # зважені вхідні параметри прошарку 2 на вихідний прошарок
    a2 = sigmoid(z2)  # вихідні параметри нейромережі

    return (a2)


# ------- ініціалізація початкових значення вагових коефіціентів мережі методом рандомізації
def generate_wt(x, y):
    l = []
    for i in range(x * y):
        l.append(np.random.randn())
    return (np.array(l).reshape(x, y))


# ------- контроль навченості мережі за середньоквадратичною помилкою mean square error(MSE)
def loss(out, Y):
    s = (np.square(out - Y))
    s = np.sum(s) / len(y)
    return (s)


# ------- зворотне поширення помилки -------------------------------------------------------
def back_prop(x, y, w1, w2, alpha):
    # прихований прошарок
    z1 = x.dot(w1)  # зважені вхідні параметри вхідного прошарку 1
    a1 = sigmoid(z1)  # аддитивна згортка - вихід з прошарку 1 - вхід прошарку 2

    # вихідний прошарок
    z2 = a1.dot(w2)  # зважені вхідні параметри прошарку 2 на вихідний прошарок
    a2 = sigmoid(z2)  # вихідні параметри нейромережі

    # похибка на вихідному прошарку
    d2 = (a2 - y)
    d1 = np.multiply((w2.dot((d2.transpose()))).transpose(),
                     (np.multiply(a1, 1 - a1)))

    # градієнт для w1 і w2
    w1_adj = x.transpose().dot(d1)
    w2_adj = a1.transpose().dot(d2)

    # оновлення параметрів з контролем помилки alpha
    w1 = w1 - (alpha * (w1_adj))
    w2 = w2 - (alpha * (w2_adj))

    return (w1, w2)


# ------- тренування мережі з контролем помилки alpha на epoch -----------------------------
def train(x, Y, w1, w2, alpha=0.01, epoch=10):
    acc = []
    losss = []
    for j in range(epoch):
        l = []
        for i in range(len(x)):
            out = f_forward(x[i], w1, w2)
            l.append((loss(out, Y[i])))
            w1, w2 = back_prop(x[i], y[i], w1, w2, alpha)
        print("epochs:", j + 1, "======== acc:", (1 - (sum(l) / len(x))) * 100)
        acc.append((1 - (sum(l) / len(x))) * 100)
        losss.append(sum(l) / len(x))
    return (acc, losss, w1, w2)


# ------- ідентифікація літералів / передбачення ------------------------------------------
def predict(x, w1, w2):
    '''
    Функція прогнозу прийматиме такі аргументи:
    :param x: матриця зображення
    :param w1: треновані ваги
    :param w2: треновані ваги
    :return: відображає ідентифікований літерал - графічну формиу
    '''

    Out = f_forward(x, w1, w2)
    maxm = 0
    k = 0
    for i in range(len(Out[0])):
        if (maxm < Out[0][i]):
            maxm = Out[0][i]
            k = i

    if (k == 0):
        print("Image of symbol &.", '\n')
    elif (k == 1):
        print("Image of symbol ∧.", '\n')
    elif (k == 2):
        print("Image of symbol ∨.", '\n')
    elif (k == 3):
        print("Image of symbol ↵.", '\n')
    elif (k == 4):
        print("Image of symbol ∑.", '\n')
    else:
        print("Image of symbol ≡.", '\n')
    plt.imshow(x.reshape(8, 6))
    plt.show()

    return


if __name__ == '__main__':
    # ------------------- вхідні дані -----------------------------------
    x = data_x()
    y = data_y()
    print('DataSet-масив: навчальна пара для навчання із вчителем')
    print('х = ', x, '\n')
    print('y = ', y, '\n')

    # --- ініціалізація вагових коєфіцієнтів на 2 прошарка з відповідним до характеристик прошарків складом параметрів
    w1 = generate_wt(48, 6)
    w2 = generate_wt(6, 6)

    print('ініціалізація вагових коєфіцієнтів на 2 прошарка')
    # print('(w1 = ', w1, '\n')
    # print('w2 = ', w2, '\n')

    # ------- тренування мережі з контролем помилки alpha на epoch -------
    print('тренування мережі з контролем помилки alpha на epoch')
    acc, losss, w1, w2 = train(x, y, w1, w2, 0.1, 70)

    # натреновані вагови коефіцієнти
    # print('натреновані вагови коефіцієнти  на 2 прошарка')
    # print('(w1 = ', w1, '\n')
    # print('w2 = ', w2, '\n')

    # -------------- контроль / візуалізація параметрів тренування -------
    # точність
    plt.plot(acc)
    plt.ylabel('Точність')
    plt.xlabel("Епохи:")
    plt.show()
    # втрати
    plt.plot(losss)
    plt.ylabel('Втрати')
    plt.xlabel("Епохи:")
    plt.show()

    # ------- ідентифікація символів / передбачення ---------------------
    print('Вхідні параметри відповідають символу "&"')
    print('Результат ідентифікації:')
    predict(x[0], w1, w2)

    print('Вхідні параметри відповідають символу "∧"')
    print('Результат ідентифікації:')
    predict(x[1], w1, w2)

    print('Вхідні параметри відповідають символу "∨"')
    print('Результат ідентифікації:')
    predict(x[2], w1, w2)

    print('Вхідні параметри відповідають символу "↵"')
    print('Результат ідентифікації:')
    predict(x[3], w1, w2)

    print('Вхідні параметри відповідають символу "∑"')
    print('Результат ідентифікації:')
    predict(x[4], w1, w2)

    print('Вхідні параметри відповідають символу "≡"')
    print('Результат ідентифікації:')
    predict(x[5], w1, w2)